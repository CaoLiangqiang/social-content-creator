#!/usr/bin/env python3
"""
ç¤¾äº¤å†…å®¹çˆ¬è™« - å®Œæ•´ç‰ˆ

> ğŸš€ ä¸‰å¹³å°å†…å®¹çˆ¬å–ä¸åˆ†æç³»ç»Ÿ
> å¼€å‘è€…: æ™ºå® (AIåŠ©æ‰‹)
>
> åŠŸèƒ½ï¼š
> - çˆ¬å–Bç«™ã€æŠ–éŸ³ã€å°çº¢ä¹¦çš„å†…å®¹
> - æ”¯æŒå•ä¸ªæˆ–æ‰¹é‡URL
> - è‡ªåŠ¨å¯¼å‡ºJSONã€CSVã€MarkdownæŠ¥å‘Š
> - æ•°æ®åˆ†æå’Œç»Ÿè®¡
"""

import asyncio
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.crawler.bilibili.bilibili_crawler import BilibiliCrawler
from src.crawler.douyin.douyin_crawler import DouyinCrawler
from src.utils.data_exporter import DataExporter


class SocialContentCrawler:
    """ç¤¾äº¤å†…å®¹çˆ¬è™«ä¸»ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.bilibili_crawler = BilibiliCrawler()
        self.douyin_crawler = DouyinCrawler()
        self.exporter = DataExporter(project_root / 'exports')

        self.results = {
            'bilibili': [],
            'douyin': [],
            'xiaohongshu': []
        }

    async def crawl_bilibili(self, url: str) -> Optional[Dict]:
        """çˆ¬å–Bç«™å†…å®¹"""
        try:
            import aiohttp
            import re

            print(f"\nğŸ¬ çˆ¬å–Bç«™å†…å®¹...")
            print(f"URL: {url}")

            # è§£æçŸ­é“¾æ¥
            async with aiohttp.ClientSession() as session:
                async with session.head(url, allow_redirects=True) as response:
                    real_url = str(response.url)

            # æå–BVID
            bvid_match = re.search(r'BV[a-zA-Z0-9]{10}', real_url)
            if not bvid_match:
                print("âŒ æ— æ³•æå–BVID")
                return None

            bvid = bvid_match.group(0)
            print(f"BVID: {bvid}")

            # çˆ¬å–
            video_data = await self.bilibili_crawler.crawl_video_full(bvid)

            if video_data and video_data.get('video_info'):
                video_info = video_data['video_info']

                result = {
                    'platform': 'Bç«™',
                    'url': url,
                    'bvid': bvid,
                    'title': video_info.get('title', 'N/A'),
                    'desc': video_info.get('desc', 'N/A'),
                    'play_count': video_info.get('play_count', 0),
                    'like_count': video_info.get('like_count', 0),
                    'coin_count': video_info.get('coin_count', 0),
                    'favorite_count': video_info.get('favorite_count', 0),
                    'author': video_info.get('author', 'N/A'),
                    'duration': video_info.get('length', 0),
                    'crawled_at': datetime.now().isoformat()
                }

                self.results['bilibili'].append(result)
                print(f"âœ… çˆ¬å–æˆåŠŸ: {result['title'][:50]}")
                return result
            else:
                print("âŒ çˆ¬å–å¤±è´¥")
                return None

        except Exception as e:
            print(f"âŒ Bç«™çˆ¬å–å¤±è´¥: {e}")
            return None

    async def crawl_douyin(self, url: str) -> Optional[Dict]:
        """çˆ¬å–æŠ–éŸ³å†…å®¹"""
        try:
            print(f"\nğŸµ çˆ¬å–æŠ–éŸ³å†…å®¹...")
            print(f"URL: {url}")

            video = await self.douyin_crawler.crawl_video_by_url(url)

            if video:
                result = {
                    'platform': 'æŠ–éŸ³',
                    'url': url,
                    'video_id': video.video_id,
                    'title': video.title,
                    'desc': video.desc,
                    'digg_count': video.statistics.digg_count,
                    'comment_count': video.statistics.comment_count,
                    'share_count': video.statistics.share_count,
                    'collect_count': video.statistics.collect_count,
                    'play_count': video.statistics.play_count,
                    'author': video.author.nickname,
                    'author_follower': video.author.follower_count,
                    'duration': video.video.duration,
                    'tags': ', '.join([t.get('hashtag_name', '') for t in video.text_extra]),
                    'crawled_at': datetime.now().isoformat()
                }

                self.results['douyin'].append(result)
                print(f"âœ… çˆ¬å–æˆåŠŸ: {result['title'][:50]}")
                return result
            else:
                print("âŒ çˆ¬å–å¤±è´¥")
                return None

        except Exception as e:
            print(f"âŒ æŠ–éŸ³çˆ¬å–å¤±è´¥: {e}")
            return None

    async def crawl_batch(self, urls: Dict[str, str]) -> Dict:
        """æ‰¹é‡çˆ¬å–

        Args:
            urls: å¹³å°åˆ°URLçš„æ˜ å°„

        Returns:
            çˆ¬å–ç»“æœ
        """
        print("="*70)
        print("å¼€å§‹æ‰¹é‡çˆ¬å–")
        print("="*70)

        tasks = []

        # Bç«™
        if 'bilibili' in urls and urls['bilibili']:
            tasks.append(self.crawl_bilibili(urls['bilibili']))

        # æŠ–éŸ³
        if 'douyin' in urls and urls['douyin']:
            tasks.append(self.crawl_douyin(urls['douyin']))

        # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        await asyncio.gather(*tasks)

        print("\n" + "="*70)
        print("æ‰¹é‡çˆ¬å–å®Œæˆ")
        print("="*70)

        return self.results

    def export_all(self, filename_prefix: str = None) -> Dict[str, Path]:
        """å¯¼å‡ºæ‰€æœ‰æ•°æ®

        Args:
            filename_prefix: æ–‡ä»¶åå‰ç¼€

        Returns:
            å„æ ¼å¼å¯¼å‡ºè·¯å¾„
        """
        if not filename_prefix:
            filename_prefix = datetime.now().strftime('%Y%m%d_%H%M%S')

        paths = {}

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        all_data = []
        all_data.extend(self.results['bilibili'])
        all_data.extend(self.results['douyin'])

        # å¯¼å‡ºJSON
        json_path = self.exporter.export_json(
            all_data,
            f"{filename_prefix}_data"
        )
        paths['json'] = json_path

        # å¯¼å‡ºCSV
        csv_path = self.exporter.export_csv(
            all_data,
            f"{filename_prefix}_data"
        )
        paths['csv'] = csv_path

        # å¯¼å‡ºMarkdownæŠ¥å‘Š
        md_path = self.exporter.export_excel_report(
            bilibili_data=self.results['bilibili'][0] if self.results['bilibili'] else None,
            douyin_data=self.results['douyin'][0] if self.results['douyin'] else None,
            filename=f"{filename_prefix}_report"
        )
        paths['markdown'] = md_path

        return paths

    def print_summary(self):
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ“Š çˆ¬å–ç»Ÿè®¡")
        print("="*70)

        total = len(self.results['bilibili']) + len(self.results['douyin'])

        print(f"Bç«™: {len(self.results['bilibili'])} æ¡")
        print(f"æŠ–éŸ³: {len(self.results['douyin'])} æ¡")
        print(f"æ€»è®¡: {total} æ¡")

        # ç»Ÿè®¡æ€»æ•°
        total_plays = 0
        total_likes = 0

        for item in self.results['bilibili']:
            total_plays += item.get('play_count', 0)
            total_likes += item.get('like_count', 0)

        for item in self.results['douyin']:
            total_plays += item.get('play_count', 0)
            total_likes += item.get('digg_count', 0)

        print(f"\næ€»æ’­æ”¾é‡: {total_plays:,}")
        print(f"æ€»ç‚¹èµæ•°: {total_likes:,}")

        print("="*70)


async def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     ç¤¾äº¤å†…å®¹çˆ¬è™«ç³»ç»Ÿ - æ™ºå®å‡ºå“ ğŸŒ¸                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

åŸºäºçœŸå®URLçš„é›†æˆæµ‹è¯•
    """)

    # ç”¨æˆ·æä¾›çš„çœŸå®URL
    urls = {
        'bilibili': 'https://b23.tv/gp9M5rR',
        'douyin': 'https://v.douyin.com/arLquTQPBYM/'
    }

    crawler = SocialContentCrawler()

    # æ‰¹é‡çˆ¬å–
    await crawler.crawl_batch(urls)

    # æ‰“å°ç»Ÿè®¡
    crawler.print_summary()

    # å¯¼å‡ºæ•°æ®
    print("\nå¯¼å‡ºæ•°æ®...")
    paths = crawler.export_all()

    print("\nğŸ“ å¯¼å‡ºæ–‡ä»¶:")
    for format_type, path in paths.items():
        print(f"  {format_type:10s}: {path}")

    print("\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("  - JSON: å¯ç”¨äºç¨‹åºå¤„ç†")
    print("  - CSV: å¯ç”¨äºExcelåˆ†æ")
    print("  - Markdown: å¯ç”¨äºé˜…è¯»å’Œåˆ†äº«")


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
