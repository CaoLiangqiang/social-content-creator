"""
å®Œæ•´çš„çˆ¬è™«ç¤ºä¾‹

æ•´åˆçˆ¬è™«ã€æ•°æ®å­˜å‚¨ã€æ•°æ®ç®¡é“çš„å®Œæ•´ç¤ºä¾‹
"""

import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.crawler.xiaohongshu import XiaohongshuCrawler
from src.storage import (
    init_database,
    init_mongodb,
    close_database,
    close_mongodb,
    crawler_job_dao,
    job_pipeline
)


async def crawl_keyword_example(keyword: str, limit: int = 50):
    """
    å…³é”®è¯çˆ¬å–ç¤ºä¾‹
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        limit: æœ€å¤§çˆ¬å–æ•°é‡
    """
    print(f"\n{'='*60}")
    print(f"å¼€å§‹çˆ¬å–å…³é”®è¯: {keyword}")
    print(f"{'='*60}\n")
    
    # 1. åˆå§‹åŒ–æ•°æ®åº“
    print("ğŸ“¦ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
    await init_database()
    await init_mongodb()
    print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    
    # 2. åˆ›å»ºçˆ¬è™«ä»»åŠ¡
    print("ğŸ“‹ åˆ›å»ºçˆ¬è™«ä»»åŠ¡...")
    job_id = await crawler_job_dao.create_job(
        platform_id=1,  # å°çº¢ä¹¦
        job_type='keyword_search',
        target=keyword,
        max_items=limit
    )
    print(f"âœ… ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {job_id}\n")
    
    # 3. åˆå§‹åŒ–çˆ¬è™«
    print("ğŸ•·ï¸ åˆå§‹åŒ–çˆ¬è™«...")
    crawler = XiaohongshuCrawler()
    
    # è®¾ç½®Cookieï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    # cookie = os.getenv('XIAOHONGSHU_COOKIE')
    # if cookie:
    #     crawler.set_cookie(cookie)
    #     print("âœ… Cookieå·²è®¾ç½®")
    
    print("âœ… çˆ¬è™«åˆå§‹åŒ–å®Œæˆ\n")
    
    # 4. å®šä¹‰çˆ¬è™«å‡½æ•°
    async def crawl_func():
        print(f"ğŸ” å¼€å§‹æœç´¢: {keyword}")
        contents = await crawler.crawl_by_keyword(keyword, limit=limit)
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(contents)} æ¡ç¬”è®°\n")
        return contents
    
    # 5. æ‰§è¡Œä»»åŠ¡
    print("ğŸš€ æ‰§è¡Œçˆ¬è™«ä»»åŠ¡...")
    try:
        await job_pipeline.execute_job(job_id, crawl_func)
    except Exception as e:
        print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
        return
    
    # 6. æ˜¾ç¤ºç»“æœ
    print(f"\n{'='*60}")
    print("ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
    print(f"{'='*60}\n")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = crawler.get_stats()
    print(f"ğŸ“Š çˆ¬è™«ç»Ÿè®¡:")
    print(f"  - æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"  - æˆåŠŸè¯·æ±‚: {stats['success_requests']}")
    print(f"  - å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")
    print(f"  - æˆåŠŸç‡: {stats['success_rate']}%")
    print(f"  - è¿è¡Œæ—¶é—´: {stats['runtime_seconds']:.2f}ç§’")
    print(f"  - è¯·æ±‚é€Ÿç‡: {stats['requests_per_second']:.2f} req/s\n")
    
    # æ˜¾ç¤ºæ•°æ®ç®¡é“ç»Ÿè®¡
    pipeline_stats = job_pipeline.data_pipeline.get_stats()
    print(f"ğŸ“Š æ•°æ®ç®¡é“ç»Ÿè®¡:")
    print(f"  - æ€»å¤„ç†æ•°: {pipeline_stats['total_processed']}")
    print(f"  - æˆåŠŸæ•°: {pipeline_stats['success_count']}")
    print(f"  - å¤±è´¥æ•°: {pipeline_stats['failed_count']}\n")
    
    # 7. å…³é—­æ•°æ®åº“è¿æ¥
    print("ğŸ”’ å…³é—­æ•°æ®åº“è¿æ¥...")
    await close_database()
    await close_mongodb()
    print("âœ… å®Œæˆ\n")


async def crawl_content_detail_example(content_id: str):
    """
    çˆ¬å–ç¬”è®°è¯¦æƒ…ç¤ºä¾‹
    
    Args:
        content_id: ç¬”è®°ID
    """
    print(f"\n{'='*60}")
    print(f"çˆ¬å–ç¬”è®°è¯¦æƒ…: {content_id}")
    print(f"{'='*60}\n")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    await init_database()
    await init_mongodb()
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = XiaohongshuCrawler()
    
    # çˆ¬å–è¯¦æƒ…
    content = await crawler.crawl_content_detail(content_id)
    
    if content:
        print("âœ… çˆ¬å–æˆåŠŸ\n")
        print(f"æ ‡é¢˜: {content['title']}")
        print(f"ä½œè€…: {content['author_name']}")
        print(f"å†…å®¹: {content['content'][:100]}...")
        print(f"ç‚¹èµ: {content['like_count']}")
        print(f"æ”¶è—: {content['collect_count']}")
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        await job_pipeline.data_pipeline.process_content(content)
        print("\nâœ… å·²ä¿å­˜åˆ°æ•°æ®åº“")
    else:
        print("âŒ çˆ¬å–å¤±è´¥")
    
    # å…³é—­æ•°æ®åº“
    await close_database()
    await close_mongodb()


async def crawl_user_example(user_id: str):
    """
    çˆ¬å–ç”¨æˆ·ä¿¡æ¯ç¤ºä¾‹
    
    Args:
        user_id: ç”¨æˆ·ID
    """
    print(f"\n{'='*60}")
    print(f"çˆ¬å–ç”¨æˆ·ä¿¡æ¯: {user_id}")
    print(f"{'='*60}\n")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    await init_database()
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = XiaohongshuCrawler()
    
    # çˆ¬å–ç”¨æˆ·ä¿¡æ¯
    user = await crawler.crawl_user_info(user_id)
    
    if user:
        print("âœ… çˆ¬å–æˆåŠŸ\n")
        print(f"ç”¨æˆ·å: {user['username']}")
        print(f"ç®€ä»‹: {user['bio'][:100]}...")
        print(f"ç²‰ä¸: {user['follower_count']}")
        print(f"å…³æ³¨: {user['following_count']}")
        print(f"ç¬”è®°æ•°: {user['note_count']}")
    else:
        print("âŒ çˆ¬å–å¤±è´¥")
    
    # å…³é—­æ•°æ®åº“
    await close_database()


async def batch_crawl_example():
    """
    æ‰¹é‡çˆ¬å–ç¤ºä¾‹
    
    åŒæ—¶çˆ¬å–å¤šä¸ªå…³é”®è¯
    """
    keywords = ['ç¾é£Ÿ', 'æ—…è¡Œ', 'ç©¿æ­', 'æ•°ç ', 'è¯»ä¹¦']
    
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡çˆ¬å–ç¤ºä¾‹")
    print(f"å…³é”®è¯: {', '.join(keywords)}")
    print(f"{'='*60}\n")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    await init_database()
    await init_mongodb()
    
    # åˆ›å»ºçˆ¬è™«
    crawler = XiaohongshuCrawler()
    
    # å¹¶å‘çˆ¬å–ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
    semaphore = asyncio.Semaphore(2)  # æœ€å¤š2ä¸ªå¹¶å‘
    
    async def crawl_with_limit(keyword):
        async with semaphore:
            print(f"\nğŸ” å¼€å§‹çˆ¬å–: {keyword}")
            contents = await crawler.crawl_by_keyword(keyword, limit=20)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            content_ids = await job_pipeline.data_pipeline.process_contents_batch(
                contents,
                save_raw=True
            )
            
            print(f"âœ… {keyword}: çˆ¬å– {len(contents)} æ¡ï¼Œä¿å­˜ {len(content_ids)} æ¡")
            return len(contents)
    
    # æ‰§è¡Œæ‰¹é‡çˆ¬å–
    tasks = [crawl_with_limit(kw) for kw in keywords]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # æ˜¾ç¤ºç»“æœ
    total = sum(r for r in results if isinstance(r, int))
    print(f"\n{'='*60}")
    print(f"æ‰¹é‡çˆ¬å–å®Œæˆï¼Œå…±çˆ¬å– {total} æ¡ç¬”è®°")
    print(f"{'='*60}\n")
    
    # æ˜¾ç¤ºæ€»ç»Ÿè®¡
    stats = crawler.get_stats()
    print(f"ğŸ“Š æ€»ç»Ÿè®¡:")
    print(f"  - æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
    print(f"  - æˆåŠŸè¯·æ±‚: {stats['success_requests']}")
    print(f"  - æˆåŠŸç‡: {stats['success_rate']}%")
    
    # å…³é—­æ•°æ®åº“
    await close_database()
    await close_mongodb()


async def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "="*60)
    print("ç¤¾äº¤å†…å®¹åˆ›ä½œå¹³å° - çˆ¬è™«ç¤ºä¾‹")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    if not os.getenv('DB_PASSWORD'):
        print("\nâš ï¸  è­¦å‘Š: æœªè®¾ç½®æ•°æ®åº“å¯†ç ")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®DB_PASSWORD\n")
    
    # é€‰æ‹©ç¤ºä¾‹
    print("\nè¯·é€‰æ‹©ç¤ºä¾‹:")
    print("1. å…³é”®è¯çˆ¬å–")
    print("2. ç¬”è®°è¯¦æƒ…çˆ¬å–")
    print("3. ç”¨æˆ·ä¿¡æ¯çˆ¬å–")
    print("4. æ‰¹é‡çˆ¬å–")
    print("0. é€€å‡º")
    
    choice = input("\nè¯·è¾“å…¥é€‰é¡¹ (0-4): ").strip()
    
    if choice == '1':
        keyword = input("è¯·è¾“å…¥å…³é”®è¯: ").strip()
        limit = input("è¯·è¾“å…¥æ•°é‡ (é»˜è®¤50): ").strip()
        limit = int(limit) if limit else 50
        
        await crawl_keyword_example(keyword, limit)
    
    elif choice == '2':
        content_id = input("è¯·è¾“å…¥ç¬”è®°ID: ").strip()
        await crawl_content_detail_example(content_id)
    
    elif choice == '3':
        user_id = input("è¯·è¾“å…¥ç”¨æˆ·ID: ").strip()
        await crawl_user_example(user_id)
    
    elif choice == '4':
        confirm = input("ç¡®è®¤æ‰§è¡Œæ‰¹é‡çˆ¬å–ï¼Ÿ(y/n): ").strip().lower()
        if confirm == 'y':
            await batch_crawl_example()
    
    else:
        print("é€€å‡ºç¨‹åº")


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºå·²ä¸­æ–­")
    except Exception as e:
        print(f"\n\nâŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
