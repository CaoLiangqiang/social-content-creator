#!/usr/bin/env python3
"""
Bç«™æ’è¡Œæ¦œçˆ¬è™«

> ğŸ¬ çˆ¬å–Bç«™æ’è¡Œæ¦œå¹¶åˆ†æå†…å®¹
> å¼€å‘è€…: æ™ºå® (AIåŠ©æ‰‹)
"""

import asyncio
import sys
import aiohttp
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.crawler.bilibili.bilibili_crawler import BilibiliCrawler


class BilibiliRankCrawler:
    """Bç«™æ’è¡Œæ¦œçˆ¬è™«"""

    def __init__(self):
        self.crawler = BilibiliCrawler()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        self.session = aiohttp.ClientSession(headers=headers)

    async def get_ranking(self, rid: int = 1, num: int = 10) -> List[Dict]:
        """
        è·å–æ’è¡Œæ¦œ

        Args:
            rid: åˆ†åŒºID (1=å…¨ç«™, 3=éŸ³ä¹, 11=æ•°ç ç­‰)
            num: æ•°é‡

        Returns:
            æ’è¡Œæ¦œæ•°æ®åˆ—è¡¨
        """
        url = "https://api.bilibili.com/x/web-interface/ranking/v2"
        params = {
            'rid': rid,
            'type': 'all',
            'arc_type': 0
        }

        try:
            async with self.session.get(url, params=params) as response:
                data = await response.json()

                if data.get('code') == 0:
                    items = data['data']['list'][:num]
                    return items
                else:
                    print(f"âŒ APIé”™è¯¯: {data.get('message')}")
                    return []
        except Exception as e:
            print(f"âŒ è·å–æ’è¡Œæ¦œå¤±è´¥: {e}")
            return []

    def filter_non_entertainment(self, videos: List[Dict]) -> List[Dict]:
        """
        ç­›é€‰éå¨±ä¹æ€§è´¨çš„è§†é¢‘

        Args:
            videos: è§†é¢‘åˆ—è¡¨

        Returns:
            ç­›é€‰åçš„è§†é¢‘åˆ—è¡¨
        """
        filtered = []

        # å¨±ä¹ç›¸å…³å…³é”®è¯
        entertainment_keywords = [
            'åŠ¨æ¼«', 'åŠ¨ç”»', 'æ¼«ç”»', 'ç•ªå‰§',
            'å¨±ä¹', 'æ˜æ˜Ÿ', 'ç»¼è‰º', 'å½±è§†',
            'æ¸¸æˆ', 'ç”µç«', 'æ‰‹æ¸¸',
            'èˆè¹ˆ', 'é¬¼ç•œ', 'vlog',
            'é¢œå€¼', 'ç¾å¥³', 'å¸…å“¥'
        ]

        # éå¨±ä¹åˆ†åŒº
        tech_rids = [11, 95, 230]  # æ•°ç ã€çŸ¥è¯†ã€ç§‘æŠ€
        education_rids = [36, 201, 124]  # æ•™è‚²ã€æŠ€èƒ½ã€è¯­è¨€å­¦ä¹ 
        info_rids = [3, 129, 232]  # éŸ³ä¹ï¼ˆéƒ¨åˆ†èµ„è®¯ç±»ï¼‰

        for video in videos:
            title = video.get('title', '').lower()
            desc = video.get('description', '').lower()
            tid = video.get('tid', 0)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¨±ä¹å…³é”®è¯
            is_entertainment = any(kw in title or kw in desc for kw in entertainment_keywords)

            # æ£€æŸ¥åˆ†åŒºï¼ˆç§‘æŠ€ã€æ•™è‚²ã€èµ„è®¯ç­‰ï¼‰
            is_tech_education = tid in tech_rids + education_rids

            if not is_entertainment or is_tech_education:
                filtered.append(video)

        return filtered

    async def analyze_videos(self, videos: List[Dict], max_count: int = 10) -> List[Dict]:
        """
        åˆ†æè§†é¢‘å†…å®¹

        Args:
            videos: è§†é¢‘åˆ—è¡¨
            max_count: æœ€å¤§åˆ†ææ•°é‡

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []

        for i, video in enumerate(videos[:max_count], 1):
            print(f"\n[{i}/{len(videos[:max_count])}] åˆ†æè§†é¢‘: {video.get('title', '')[:50]}")

            bvid = video.get('bvid')

            if not bvid:
                continue

            # çˆ¬å–è¯¦ç»†æ•°æ®
            try:
                video_data = await self.crawler.crawl_video_full(bvid)

                if video_data and video_data.get('video_info'):
                    info = video_data['video_info']

                    result = {
                        'rank': i,
                        'bvid': bvid,
                        'title': info.get('title', ''),
                        'desc': info.get('desc', '')[:200],
                        'author': info.get('author', ''),
                        'play_count': info.get('play_count', 0),
                        'like_count': info.get('like_count', 0),
                        'coin_count': info.get('coin_count', 0),
                        'favorite_count': info.get('favorite_count', 0),
                        'duration': info.get('length', 0),
                        'category': video.get('tname', ''),
                        'pubdate': datetime.fromtimestamp(video.get('pubdate', 0)).strftime('%Y-%m-%d %H:%M')
                    }

                    results.append(result)
                    print(f"  âœ… å®Œæˆ: {result['title'][:30]}")

                else:
                    print(f"  âš ï¸ æ•°æ®ä¸å®Œæ•´")

            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")

        return results

    async def close(self):
        """å…³é—­session"""
        await self.session.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Bç«™æ’è¡Œæ¦œåˆ†æ - æ™ºå®å‡ºå“ ğŸŒ¸                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ç›®æ ‡:
1. è·å–Bç«™å…¨ç«™æ’è¡Œæ¦œå‰10å
2. ç­›é€‰éå¨±ä¹æ€§è´¨è§†é¢‘
3. çˆ¬å–è¯¦ç»†æ•°æ®å¹¶åˆ†æ
4. ç”Ÿæˆå†…å®¹æŠ¥å‘Š
    """)

    crawler = BilibiliRankCrawler()

    try:
        # è·å–æ’è¡Œæ¦œ
        print("\n" + "="*70)
        print("æ­¥éª¤ 1: è·å–Bç«™å…¨ç«™æ’è¡Œæ¦œ")
        print("="*70)

        ranking = await crawler.get_ranking(rid=1, num=10)

        if not ranking:
            print("âŒ æœªè·å–åˆ°æ’è¡Œæ¦œæ•°æ®")
            return

        print(f"âœ… è·å–åˆ° {len(ranking)} æ¡è§†é¢‘")

        # ç­›é€‰éå¨±ä¹è§†é¢‘
        print("\n" + "="*70)
        print("æ­¥éª¤ 2: ç­›é€‰éå¨±ä¹æ€§è´¨è§†é¢‘")
        print("="*70)

        filtered = crawler.filter_non_entertainment(ranking)

        print(f"åŸæ’è¡Œæ¦œ: {len(ranking)} æ¡")
        print(f"ç­›é€‰å: {len(filtered)} æ¡")

        if len(filtered) == 0:
            print("\nâš ï¸ æœªæ‰¾åˆ°éå¨±ä¹è§†é¢‘ï¼Œæ˜¾ç¤ºå…¨éƒ¨è§†é¢‘")
            filtered = ranking[:5]  # å–å‰5ä¸ª
        else:
            print(f"\nç­›é€‰ç»“æœ:")
            for i, v in enumerate(filtered, 1):
                print(f"  {i}. {v.get('title', '')}")

        # åˆ†æè§†é¢‘
        print("\n" + "="*70)
        print("æ­¥éª¤ 3: åˆ†æè§†é¢‘å†…å®¹")
        print("="*70)

        results = await crawler.analyze_videos(filtered, max_count=10)

        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*70)
        print("æ­¥éª¤ 4: ç”ŸæˆæŠ¥å‘Š")
        print("="*70)

        report_file = project_root / 'exports' / f'ranking_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.md'
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Bç«™æ’è¡Œæ¦œå†…å®¹åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ•°æ®æ¥æº**: Bç«™å…¨ç«™æ’è¡Œæ¦œ\n")
            f.write(f"**åˆ†ææ•°é‡**: {len(results)} æ¡\n\n")
            f.write("---\n\n")

            if results:
                # æ€»ä½“ç»Ÿè®¡
                f.write("## ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
                total_plays = sum(r['play_count'] for r in results)
                total_likes = sum(r['like_count'] for r in results)
                total_coins = sum(r['coin_count'] for r in results)

                f.write(f"- æ€»æ’­æ”¾é‡: {total_plays:,}\n")
                f.write(f"- æ€»ç‚¹èµæ•°: {total_likes:,}\n")
                f.write(f"- æ€»æŠ•å¸æ•°: {total_coins:,}\n")
                f.write(f"- å¹³å‡æ’­æ”¾: {total_plays//len(results):,}\n")
                f.write(f"- å¹³å‡ç‚¹èµ: {total_likes//len(results):,}\n\n")

                # åˆ†ç±»ç»Ÿè®¡
                f.write("## ğŸ“ åˆ†ç±»ç»Ÿè®¡\n\n")
                categories = {}
                for r in results:
                    cat = r.get('category', 'å…¶ä»–')
                    categories[cat] = categories.get(cat, 0) + 1

                for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
                    f.write(f"- {cat}: {count} æ¡\n")
                f.write("\n")

                # è¯¦ç»†åˆ—è¡¨
                f.write("## ğŸ“‹ è§†é¢‘è¯¦æƒ…\n\n")

                for r in results:
                    f.write(f"### {r['rank']}. {r['title']}\n\n")
                    f.write(f"**åˆ†åŒº**: {r['category']} | **UPä¸»**: {r['author']} | **å‘å¸ƒ**: {r['pubdate']}\n\n")

                    stats = []
                    stats.append(f"æ’­æ”¾: {r['play_count']:,}")
                    stats.append(f"ç‚¹èµ: {r['like_count']:,}")
                    stats.append(f"æŠ•å¸: {r['coin_count']:,}")
                    stats.append(f"æ”¶è—: {r['favorite_count']:,}")

                    f.write(" | ".join(stats) + "\n\n")

                    if r['desc']:
                        f.write(f"**ç®€ä»‹**: {r['desc']}\n\n")

                    f.write("---\n\n")

        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

        # ä¿å­˜JSON
        json_file = project_root / 'exports' / f'ranking_data_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ•°æ®å·²ä¿å­˜: {json_file}")

        print("\n" + "="*70)
        print("ğŸ‰ åˆ†æå®Œæˆï¼")
        print("="*70)

        # æ‰“å°ç®€è¦ç»Ÿè®¡
        print(f"\nåˆ†æè§†é¢‘æ•°: {len(results)}")
        print(f"æ€»æ’­æ”¾é‡: {total_plays:,}")
        print(f"æ€»ç‚¹èµæ•°: {total_likes:,}")
        print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_file}")

    finally:
        await crawler.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
