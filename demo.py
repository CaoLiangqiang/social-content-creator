#!/usr/bin/env python3
"""
å°çº¢ä¹¦çˆ¬è™«æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨çˆ¬è™«çš„åŸºæœ¬åŠŸèƒ½
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.crawler.base.rate_limiter import RateLimiter
from src.crawler.base.proxy_pool import ProxyPool
from src.crawler.utils.logger import get_logger

async def demo_rate_limiter():
    """æ¼”ç¤ºé€Ÿç‡é™åˆ¶å™¨åŠŸèƒ½"""
    print("ğŸš€ æ¼”ç¤ºé€Ÿç‡é™åˆ¶å™¨åŠŸèƒ½")
    
    limiter = RateLimiter(3)  # 3 requests per second
    
    print("æ¨¡æ‹Ÿ5ä¸ªè¯·æ±‚ï¼ˆé™åˆ¶3ä¸ª/ç§’ï¼‰:")
    for i in range(5):
        start_time = asyncio.get_event_loop().time()
        await limiter.acquire()
        end_time = asyncio.get_event_loop().time()
        
        print(f"è¯·æ±‚ {i+1}: è€—æ—¶ {end_time - start_time:.2f}ç§’")
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå®é™…å·¥ä½œ

def demo_proxy_pool():
    """æ¼”ç¤ºä»£ç†æ± åŠŸèƒ½"""
    print("\nğŸŒ æ¼”ç¤ºä»£ç†æ± åŠŸèƒ½")
    
    pool = ProxyPool()
    
    print(f"å¯ç”¨ä»£ç†æ•°é‡: {len(pool.proxies)}")
    print(f"ä»£ç†æ± çŠ¶æ€: {pool.get_stats()}")
    
    # æ¨¡æ‹Ÿè·å–ä»£ç†
    for i in range(3):
        proxy = pool.get_proxy()
        print(f"è·å–ä»£ç† {i+1}: {proxy}")

def demo_logger():
    """æ¼”ç¤ºæ—¥å¿—åŠŸèƒ½"""
    print("\nğŸ“ æ¼”ç¤ºæ—¥å¿—åŠŸèƒ½")
    
    logger = get_logger('demo')
    
    logger.info("è¿™æ˜¯ä¸€æ¡ä¿¡æ¯æ—¥å¿—")
    logger.warning("è¿™æ˜¯ä¸€æ¡è­¦å‘Šæ—¥å¿—")
    logger.error("è¿™æ˜¯ä¸€æ¡é”™è¯¯æ—¥å¿—")
    
    logger.info("æµ‹è¯•å¸¦æ•°æ®çš„æ—¥å¿—", {"key": "value", "count": 42})

def demo_spider_info():
    """æ¼”ç¤ºçˆ¬è™«ä¿¡æ¯"""
    print("\nğŸ•·ï¸ å°çº¢ä¹¦çˆ¬è™«ä¿¡æ¯")
    
    info = {
        "ç¬”è®°çˆ¬è™«": {
            "åŠŸèƒ½": "çˆ¬å–å°çº¢ä¹¦ç¬”è®°è¯¦æƒ…",
            "æ•°æ®å­—æ®µ": ["æ ‡é¢˜", "å†…å®¹", "ä½œè€…", "ç‚¹èµæ•°", "è¯„è®ºæ•°", "æ ‡ç­¾", "å›¾ç‰‡"],
            "URLæ¨¡å¼": "https://www.xiaohongshu.com/explore/[ID]"
        },
        "ç”¨æˆ·çˆ¬è™«": {
            "åŠŸèƒ½": "çˆ¬å–å°çº¢ä¹¦ç”¨æˆ·ä¿¡æ¯",
            "æ•°æ®å­—æ®µ": ["ç”¨æˆ·å", "ç®€ä»‹", "ç²‰ä¸æ•°", "å…³æ³¨æ•°", "ç¬”è®°æ•°", "è®¤è¯çŠ¶æ€"],
            "URLæ¨¡å¼": "https://www.xiaohongshu.com/user/profile/[ID]"
        },
        "è¯„è®ºçˆ¬è™«": {
            "åŠŸèƒ½": "çˆ¬å–å°çº¢ä¹¦è¯„è®º",
            "æ•°æ®å­—æ®µ": ["è¯„è®ºå†…å®¹", "è¯„è®ºä½œè€…", "ç‚¹èµæ•°", "å›å¤æ•°", "æ—¶é—´"],
            "URLæ¨¡å¼": "ä»ç¬”è®°é¡µé¢æå–è¯„è®º"
        }
    }
    
    for spider_type, details in info.items():
        print(f"\n{spider_type}:")
        for key, value in details.items():
            print(f"  {key}: {value}")

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ å°çº¢ä¹¦çˆ¬è™«æ¼”ç¤ºå¼€å§‹")
    print("=" * 50)
    
    # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
    await demo_rate_limiter()
    demo_proxy_pool()
    demo_logger()
    demo_spider_info()
    
    print("\n" + "=" * 50)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ docs/XIAOHONGSHU_CRAWLER.md")
    print("ğŸš€ è¿è¡Œçˆ¬è™«: python3 run_xiaohongshu_crawler.py")

if __name__ == "__main__":
    asyncio.run(main())