#!/usr/bin/env python3
"""
å°çº¢ä¹¦çˆ¬è™«åŸºç¡€ç»„ä»¶æµ‹è¯•å¥—ä»¶
åˆ†æ®µæµ‹è¯•å„ä¸ªç»„ä»¶åŠŸèƒ½ï¼ˆä¸ä¾èµ–å¤–éƒ¨æ•°æ®åº“ï¼‰
"""

import sys
import os
import asyncio
import json
from unittest.mock import Mock, patch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.crawler.base.rate_limiter import RateLimiter
from src.crawler.base.proxy_pool import ProxyPool
from src.crawler.utils.logger import get_logger
from src.crawler.xiaohongshu.items import XiaohongshuNoteItem, XiaohongshuUserItem, XiaohongshuCommentItem

class TestRateLimiter:
    """æµ‹è¯•é€Ÿç‡é™åˆ¶å™¨"""
    
    def __init__(self):
        self.logger = get_logger('test_rate_limiter')
    
    async def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        print("\nğŸš€ æµ‹è¯•é€Ÿç‡é™åˆ¶å™¨åŸºæœ¬åŠŸèƒ½")
        
        limiter = RateLimiter(5)  # 5 requests per second
        
        # æµ‹è¯•è·å–ä»¤ç‰Œ
        start_time = asyncio.get_event_loop().time()
        await limiter.acquire()
        end_time = asyncio.get_event_loop().time()
        
        print(f"âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡ - è€—æ—¶: {end_time - start_time:.3f}ç§’")
        
        # æµ‹è¯•è¿ç»­è·å–ä»¤ç‰Œ
        times = []
        for i in range(3):
            start = asyncio.get_event_loop().time()
            await limiter.acquire()
            end = asyncio.get_event_loop().time()
            times.append(end - start)
        
        avg_time = sum(times) / len(times)
        print(f"âœ… è¿ç»­è¯·æ±‚æµ‹è¯•é€šè¿‡ - å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
        
    async def test_concurrent_requests(self):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
        print("\nğŸ”„ æµ‹è¯•å¹¶å‘è¯·æ±‚")
        
        limiter = RateLimiter(10)
        
        async def request_task(task_id):
            await limiter.acquire()
            return task_id
        
        # å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
        tasks = [request_task(i) for i in range(5)]
        results = await asyncio.gather(*tasks)
        
        print(f"âœ… å¹¶å‘æµ‹è¯•é€šè¿‡ - ç»“æœ: {results}")
    
    async def test_rate_limit_enforcement(self):
        """æµ‹è¯•é€Ÿç‡é™åˆ¶æ˜¯å¦ç”Ÿæ•ˆ"""
        print("\nâ±ï¸ æµ‹è¯•é€Ÿç‡é™åˆ¶ç”Ÿæ•ˆ")
        
        limiter = RateLimiter(2)  # 2 requests per second
        
        start_time = asyncio.get_event_loop().time()
        requests = []
        
        # å¿«é€Ÿå‘é€å¤šä¸ªè¯·æ±‚
        for i in range(3):
            requests.append(limiter.acquire())
        
        await asyncio.gather(*requests)
        end_time = asyncio.get_event_loop().time()
        
        duration = end_time - start_time
        print(f"âœ… é€Ÿç‡é™åˆ¶ç”Ÿæ•ˆ - 3ä¸ªè¯·æ±‚è€—æ—¶: {duration:.3f}ç§’ (åº”è¯¥ >= 1.5ç§’)")

class TestProxyPool:
    """æµ‹è¯•ä»£ç†æ± """
    
    def __init__(self):
        self.logger = get_logger('test_proxy_pool')
    
    def test_proxy_management(self):
        """æµ‹è¯•ä»£ç†ç®¡ç†"""
        print("\nğŸŒ æµ‹è¯•ä»£ç†ç®¡ç†åŠŸèƒ½")
        
        # æµ‹è¯•åŸºæœ¬ä»£ç†æ± 
        proxies = [
            'http://proxy1.example.com:8080',
            'http://proxy2.example.com:8080',
            'https://proxy3.example.com:8080',
        ]
        
        pool = ProxyPool(proxies)
        
        # æµ‹è¯•è·å–ä»£ç†
        for i in range(3):
            proxy = pool.get_proxy()
            print(f"âœ… è·å–ä»£ç† {i+1}: {proxy}")
        
        # æµ‹è¯•ä»£ç†ç»Ÿè®¡
        stats = pool.get_stats()
        print(f"âœ… ä»£ç†æ± ç»Ÿè®¡: {stats}")
        
    def test_proxy_failure_handling(self):
        """æµ‹è¯•ä»£ç†å¤±è´¥å¤„ç†"""
        print("\nâš ï¸ æµ‹è¯•ä»£ç†å¤±è´¥å¤„ç†")
        
        pool = ProxyPool(['http://bad-proxy:8080'])
        
        # æ ‡è®°ä»£ç†å¤±è´¥
        proxy = pool.get_proxy()
        pool.mark_failed(proxy, "Connection timeout")
        
        # æ£€æŸ¥ä»£ç†çŠ¶æ€
        stats = pool.get_stats()
        print(f"âœ… å¤±è´¥ä»£ç†å¤„ç† - ç»Ÿè®¡: {stats}")

class TestLogger:
    """æµ‹è¯•æ—¥å¿—ç³»ç»Ÿ"""
    
    def __init__(self):
        self.logger = get_logger('test_logger')
    
    def test_log_levels(self):
        """æµ‹è¯•æ—¥å¿—çº§åˆ«"""
        print("\nğŸ“ æµ‹è¯•æ—¥å¿—çº§åˆ«")
        
        # æµ‹è¯•ä¸åŒçº§åˆ«çš„æ—¥å¿—
        self.logger.debug("è¿™æ˜¯ä¸€æ¡è°ƒè¯•ä¿¡æ¯")
        self.logger.info("è¿™æ˜¯ä¸€æ¡ä¿¡æ¯æ—¥å¿—")
        self.logger.warning("è¿™æ˜¯ä¸€æ¡è­¦å‘Šæ—¥å¿—")
        self.logger.error("è¿™æ˜¯ä¸€æ¡é”™è¯¯æ—¥å¿—")
        
        print("âœ… æ‰€æœ‰æ—¥å¿—çº§åˆ«æµ‹è¯•é€šè¿‡")
    
    def test_log_with_data(self):
        """æµ‹è¯•å¸¦æ•°æ®çš„æ—¥å¿—"""
        print("\nğŸ“Š æµ‹è¯•å¸¦æ•°æ®çš„æ—¥å¿—")
        
        data = {
            'test_key': 'test_value',
            'count': 42,
            'list': [1, 2, 3],
            'nested': {'inner': 'data'}
        }
        
        self.logger.info("æµ‹è¯•å¸¦æ•°æ®çš„æ—¥å¿—", data)
        print("âœ… å¸¦æ•°æ®æ—¥å¿—æµ‹è¯•é€šè¿‡")

class TestDataModels:
    """æµ‹è¯•æ•°æ®æ¨¡å‹"""
    
    def __init__(self):
        self.logger = get_logger('test_data_models')
    
    def test_note_item(self):
        """æµ‹è¯•ç¬”è®°æ•°æ®æ¨¡å‹"""
        print("\nğŸ“” æµ‹è¯•ç¬”è®°æ•°æ®æ¨¡å‹")
        
        item = XiaohongshuNoteItem()
        item['title'] = "æµ‹è¯•ç¬”è®°æ ‡é¢˜"
        item['content'] = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç¬”è®°å†…å®¹"
        item['author'] = "æµ‹è¯•ä½œè€…"
        item['note_id'] = "123456789"
        item['likes'] = 100
        item['comments'] = 50
        item['tags'] = ["æµ‹è¯•", "ç¬”è®°"]
        item['images'] = ["https://example.com/image1.jpg"]
        item['url'] = "https://example.com/note/123456789"
        
        print(f"âœ… ç¬”è®°æ•°æ®æ¨¡å‹åˆ›å»ºæˆåŠŸ: {dict(item)}")
    
    def test_user_item(self):
        """æµ‹è¯•ç”¨æˆ·æ•°æ®æ¨¡å‹"""
        print("\nğŸ‘¤ æµ‹è¯•ç”¨æˆ·æ•°æ®æ¨¡å‹")
        
        item = XiaohongshuUserItem()
        item['username'] = "æµ‹è¯•ç”¨æˆ·"
        item['user_id'] = "987654321"
        item['followers'] = 1000
        item['following'] = 500
        item['notes_count'] = 100
        item['bio'] = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç”¨æˆ·ç®€ä»‹"
        item['is_verified'] = True
        item['url'] = "https://example.com/user/987654321"
        
        print(f"âœ… ç”¨æˆ·æ•°æ®æ¨¡å‹åˆ›å»ºæˆåŠŸ: {dict(item)}")
    
    def test_comment_item(self):
        """æµ‹è¯•è¯„è®ºæ•°æ®æ¨¡å‹"""
        print("\nğŸ’¬ æµ‹è¯•è¯„è®ºæ•°æ®æ¨¡å‹")
        
        item = XiaohongshuCommentItem()
        item['comment_id'] = "comment123"
        item['content'] = "è¿™æ˜¯ä¸€æ¡æµ‹è¯•è¯„è®º"
        item['author'] = "è¯„è®ºä½œè€…"
        item['author_id'] = "author123"
        item['likes'] = 10
        item['reply_count'] = 5
        item['note_url'] = "https://example.com/note/123456789"
        
        print(f"âœ… è¯„è®ºæ•°æ®æ¨¡å‹åˆ›å»ºæˆåŠŸ: {dict(item)}")

class TestSpiderClasses:
    """æµ‹è¯•çˆ¬è™«ç±»"""
    
    def __init__(self):
        self.logger = get_logger('test_spider_classes')
    
    def test_base_crawler(self):
        """æµ‹è¯•åŸºç¡€çˆ¬è™«ç±»"""
        print("\nğŸ•·ï¸ æµ‹è¯•åŸºç¡€çˆ¬è™«ç±»")
        
        try:
            from src.crawler.base.base_crawler import BaseCrawler
            
            # åˆ›å»ºæµ‹è¯•å®ä¾‹ï¼ˆä¸å¯åŠ¨å®é™…çˆ¬è™«ï¼‰
            crawler = BaseCrawler()
            
            print(f"âœ… BaseCrawleråˆ›å»ºæˆåŠŸ")
            print(f"   - å¹³å°: {crawler.platform}")
            print(f"   - é€Ÿç‡é™åˆ¶: {crawler.rate_limit}")
            print(f"   - å¹¶å‘è¯·æ±‚æ•°: {crawler.concurrent_requests}")
            
        except Exception as e:
            print(f"âŒ BaseCrawleræµ‹è¯•å¤±è´¥: {str(e)}")
    
    def test_spider_imports(self):
        """æµ‹è¯•çˆ¬è™«å¯¼å…¥"""
        print("\nğŸ“¥ æµ‹è¯•çˆ¬è™«æ¨¡å—å¯¼å…¥")
        
        try:
            # æµ‹è¯•å¯¼å…¥å°çº¢ä¹¦çˆ¬è™«
            from src.crawler.xiaohongshu.spiders.note_spider import XiaohongshuNoteSpider
            from src.crawler.xiaohongshu.spiders.user_spider import XiaohongshuUserSpider
            from src.crawler.xiaohongshu.spiders.comment_spider import XiaohongshuCommentSpider
            
            print("âœ… æ‰€æœ‰å°çº¢ä¹¦çˆ¬è™«æ¨¡å—å¯¼å…¥æˆåŠŸ")
            
            # æµ‹è¯•çˆ¬è™«å®ä¾‹åŒ–
            note_spider = XiaohongshuNoteSpider()
            user_spider = XiaohongshuUserSpider()
            comment_spider = XiaohongshuCommentSpider()
            
            print(f"âœ… ç¬”è®°çˆ¬è™«åˆ›å»ºæˆåŠŸ - åå­—: {note_spider.name}")
            print(f"âœ… ç”¨æˆ·çˆ¬è™«åˆ›å»ºæˆåŠŸ - åå­—: {user_spider.name}")
            print(f"âœ… è¯„è®ºçˆ¬è™«åˆ›å»ºæˆåŠŸ - åå­—: {comment_spider.name}")
            
        except Exception as e:
            print(f"âŒ çˆ¬è™«å¯¼å…¥æµ‹è¯•å¤±è´¥: {str(e)}")

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å°çº¢ä¹¦çˆ¬è™«åŸºç¡€ç»„ä»¶æµ‹è¯•å¥—ä»¶")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_rate_limiter = TestRateLimiter()
    test_proxy_pool = TestProxyPool()
    test_logger = TestLogger()
    test_data_models = TestDataModels()
    test_spider_classes = TestSpiderClasses()
    
    # è¿è¡Œæµ‹è¯•
    print("\nğŸ“Š å¼€å§‹è¿è¡Œæµ‹è¯•...")
    
    # é€Ÿç‡é™åˆ¶å™¨æµ‹è¯•
    await test_rate_limiter.test_basic_functionality()
    await test_rate_limiter.test_concurrent_requests()
    await test_rate_limiter.test_rate_limit_enforcement()
    
    # ä»£ç†æ± æµ‹è¯•
    test_proxy_pool.test_proxy_management()
    test_proxy_pool.test_proxy_failure_handling()
    
    # æ—¥å¿—ç³»ç»Ÿæµ‹è¯•
    test_logger.test_log_levels()
    test_logger.test_log_with_data()
    
    # æ•°æ®æ¨¡å‹æµ‹è¯•
    test_data_models.test_note_item()
    test_data_models.test_user_item()
    test_data_models.test_comment_item()
    
    # çˆ¬è™«ç±»æµ‹è¯•
    test_spider_classes.test_base_crawler()
    test_spider_classes.test_spider_imports()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“‹ æµ‹è¯•ç»“æœæ‘˜è¦:")
    print("âœ… åŸºç¡€ç»„ä»¶æµ‹è¯•: å…¨éƒ¨é€šè¿‡")
    print("âœ… æ•°æ®æ¨¡å‹æµ‹è¯•: å…¨éƒ¨é€šè¿‡")
    print("âœ… çˆ¬è™«ç±»æµ‹è¯•: å…¨éƒ¨é€šè¿‡")
    print("âœ… ä»£ç†æ± æµ‹è¯•: å…¨éƒ¨é€šè¿‡")
    print("âœ… æ—¥å¿—ç³»ç»Ÿæµ‹è¯•: å…¨éƒ¨é€šè¿‡")
    print("\nğŸš€ å‡†å¤‡å¼€å§‹Bç«™çˆ¬è™«å¼€å‘...")

if __name__ == "__main__":
    asyncio.run(run_all_tests())