#!/usr/bin/env python3
"""
æŠ–éŸ³çˆ¬è™«å®ç°ï¼ˆåŸºäºrequestsï¼‰

> ğŸµ æŠ–éŸ³çˆ¬è™«å®Œæ•´å®ç°
> å¼€å‘è€…: æ™ºå® (AIåŠ©æ‰‹)
"""

import asyncio
import sys
import re
import json
import requests
from pathlib import Path
from typing import Dict, Optional, List
from bs4 import BeautifulSoup


# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

from src.crawler.douyin.items import DouyinVideoItem


class DouyinVideoCrawler:
    """æŠ–éŸ³è§†é¢‘çˆ¬è™«ï¼ˆåŸºäºrequestsï¼‰"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.douyin.com/',
        })
        
        self.stats = {
            'success': 0,
            'failed': 0,
            'total': 0
        }
    
    def crawl_video_by_url(self, url: str) -> Optional[DouyinVideoItem]:
        """
        çˆ¬å–æŠ–éŸ³è§†é¢‘
        
        Args:
            url: è§†é¢‘URL
            
        Returns:
            DouyinVideoItemå¯¹è±¡
        """
        self.stats['total'] += 1
        
        try:
            # å‘é€è¯·æ±‚
            print(f"è®¿é—®URL: {url}")
            response = self.session.get(url, allow_redirects=True, timeout=15)
            
            print(f"çŠ¶æ€ç : {response.status_code}")
            print(f"æœ€ç»ˆURL: {response.url}")
            
            if response.status_code != 200:
                print(f"âŒ HTTPçŠ¶æ€ç å¼‚å¸¸: {response.status_code}")
                self.stats['failed'] += 1
                return None
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–scriptæ ‡ç­¾ä¸­çš„æ•°æ®
            scripts = soup.find_all('script')
            print(f"æ‰¾åˆ° {len(scripts)} ä¸ªscriptæ ‡ç­¾")
            
            # æŸ¥æ‰¾åŒ…å«è§†é¢‘æ•°æ®çš„script
            for i, script in enumerate(scripts):
                script_text = script.string or ''
                
                # æŸ¥æ‰¾ç‰¹å®šæ ‡è¯†
                if 'window.__INITIAL_STATE__' in script_text or 'videoData' in script_text:
                    print(f"\\nScript #{i} åŒ…å«è§†é¢‘æ•°æ®")
                    
                    # å°è¯•æå–JSON
                    video_data = self._extract_video_data(script_text)
                    
                    if video_data:
                        self.stats['success'] += 1
                        return video_data
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
            print("\\nå°è¯•ä»HTMLç›´æ¥æå–...")
            return self._extract_from_html(soup)
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            self.stats['failed'] += 1
            return None
    
    def _extract_video_data(self, script_text: str) -> Optional[DouyinVideoItem]:
        """ä»scriptæ–‡æœ¬ä¸­æå–è§†é¢‘æ•°æ®"""
        
        # æ–¹æ³•1: æå–__INITIAL_STATE__
        initial_state_match = re.search(r'window\\.__INITIAL_STATE__\\s*=\\s*(\\{.*?\\});', script_text)
        
        if initial_state_match:
            try:
                print("æ‰¾åˆ°__INITIAL_STATE__")
                data = json.loads(initial_state_match.group(1))
                
                # ä»æ•°æ®ä¸­æå–è§†é¢‘ä¿¡æ¯
                return self._parse_video_from_state(data)
            except:
                pass
        
        # æ–¹æ³•2: æŸ¥æ‰¾videoData
        video_match = re.search(r'videoData["\']?\\s*=\\s*(\\{.*?\\});', script_text)
        
        if video_match:
            try:
                print("æ‰¾åˆ°videoData")
                data = json.loads(video_match.group(1))
                return self._parse_video_from_data(data)
            except:
                pass
        
        # æ–¹æ³•3: æŸ¥æ‰¾awemeId
        aweme_match = re.search(r'"aweme_id":"?(\\d+)"?', script_text)
        if aweme_match:
            print(f"æ‰¾åˆ°aweme_id: {aweme_match.group(1)}")
            # å¯ä»¥ç”¨aweme_idæ„é€ APIè¯·æ±‚
        
        return None
    
    def _parse_video_from_state(self, data: Dict) -> Optional[DouyinVideoItem]:
        """ä»INITIAL_STATEè§£æè§†é¢‘æ•°æ®"""
        try:
            # å°è¯•ä¸åŒçš„æ•°æ®è·¯å¾„
            video_info = None
            
            # è·¯å¾„1: data.videoInfo
            if 'videoInfo' in data:
                video_info = data['videoInfo']
                print("ä»videoInfoæå–")
            
            # è·¯å¾„2: data.aweme
            elif 'aweme' in data:
                video_info = data['aweme']
                print("ä»awemeæå–")
            
            # è·¯å¾„3: data.awemeDetail
            elif 'awemeDetail' in data:
                video_info = data['awemeDetail']
                print("ä»awemeDetailæå–")
            
            if not video_info:
                print("âš ï¸ æœªæ‰¾åˆ°è§†é¢‘æ•°æ®")
                return None
            
            print(f"è§†é¢‘æ•°æ®é”®: {list(video_info.keys())}")
            return self._create_video_item(video_info)
            
        except Exception as e:
            print(f"è§£æINITIAL_STATEå¤±è´¥: {e}")
            return None
    
    def _parse_video_from_data(self, data: Dict) -> Optional[DouyinVideoItem]:
        """ä»videoDataè§£æ"""
        try:
            return self._create_video_item(data)
        except Exception as e:
            print(f"è§£ævideoDataå¤±è´¥: {e}")
            return None
    
    def _create_video_item(self, data: Dict) -> DouyinVideoItem:
        """åˆ›å»ºè§†é¢‘å¯¹è±¡"""
        
        video = DouyinVideoItem()
        
        # æå–åŸºç¡€å­—æ®µ
        video.video_id = str(data.get('aweme_id') or data.get('awemeId', ''))
        video.title = data.get('desc', '')
        video.desc = data.get('desc', '')
        
        # ç»Ÿè®¡æ•°æ®
        stats = data.get('statistics', {})
        from src.crawler.douyin.items import DouyinStatistics
        video.statistics = DouyinStatistics(
            digg_count=stats.get('digg_count', 0),
            comment_count=stats.get('comment_count', 0),
            share_count=stats.get('share_count', 0),
            play_count=stats.get('play_count', 0),
            collect_count=stats.get('collect_count', 0)
        )
        
        # ä½œè€…ä¿¡æ¯
        author_data = data.get('author', {})
        from src.crawler.douyin.items import DouyinAuthor
        video.author = DouyinAuthor(
            uid=str(author_data.get('uid', '')),
            nickname=author_data.get('nickname', ''),
            avatar_thumb=author_data.get('avatar_thumb', {}).get('url_list', [''])[0],
            signature=author_data.get('signature', ''),
            follower_count=author_data.get('follower_count', 0),
            following_count=author_data.get('following_count', 0),
            aweme_count=author_data.get('aweme_count', 0)
        )
        
        # è§†é¢‘ä¿¡æ¯
        video_data = data.get('video', {})
        from src.crawler.douyin.items import DouyinVideoInfo
        video.video = DouyinVideoInfo(
            play_addr=video_data.get('play_addr', {}).get('url_list', [''])[0],
            cover=video_data.get('cover', {}).get('url_list', [''])[0],
            duration=video_data.get('duration', 0),
            width=video_data.get('width', 0),
            height=video_data.get('height', 0)
        )
        
        return video
    
    def _extract_from_html(self, soup: BeautifulSoup) -> Optional[DouyinVideoItem]:
        """ä»HTMLç›´æ¥æå–"""
        # æŸ¥æ‰¾æ ‡é¢˜
        title_elem = soup.find('meta', property='og:title')
        title = title_elem.get('content', '') if title_elem else ''
        
        # æŸ¥æ‰¾æè¿°
        desc_elem = soup.find('meta', property='og:description')
        desc = desc_elem.get('content', '') if desc_elem else ''
        
        # æŸ¥æ‰¾è§†é¢‘URL
        video_elem = soup.find('meta', property='og:video')
        video_url = video_elem.get('content', '') if video_elem else ''
        
        if title or desc:
            print(f"\\nä»HTMLæå–:")
            print(f"  æ ‡é¢˜: {title}")
            print(f"  æè¿°: {desc[:100]}")
            print(f"  è§†é¢‘: {video_url[:100]}")
        
        return None


async def main():
    """æµ‹è¯•çˆ¬è™«"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘       ğŸµ æŠ–éŸ³çˆ¬è™«å®ç°æµ‹è¯• - æ™ºå®å‡ºå“ ğŸŒ¸              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    url = "https://v.douyin.com/arLquTQPBYM/"
    print(f"æµ‹è¯•URL: {url}\\n")
    
    crawler = DouyinVideoCrawler()
    video = crawler.crawl_video_by_url(url)
    
    if video:
        print("\\n" + "="*60)
        print("âœ… è§†é¢‘çˆ¬å–æˆåŠŸï¼")
        print("="*60)
        print(f"è§†é¢‘ID: {video.video_id}")
        print(f"æ ‡é¢˜: {video.title}")
        print(f"ç‚¹èµæ•°: {video.statistics.digg_count}")
        print(f"è¯„è®ºæ•°: {video.statistics.comment_count}")
        print(f"åˆ›ä½œè€…: {video.author.nickname}")
    else:
        print("\\nâš ï¸ è§†é¢‘çˆ¬å–å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„æ•°æ®æå–é€»è¾‘")
    
    print(f"\\nç»Ÿè®¡: {crawler.stats}")


if __name__ == "__main__":
    asyncio.run(main())
