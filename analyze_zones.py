#!/usr/bin/env python3
"""
Bç«™åˆ†åŒºå†…å®¹åˆ†æ

> ğŸ¬ åˆ†æç§‘æŠ€åŒºå’Œæ•™è‚²åŒºçš„çƒ­é—¨å†…å®¹
> å¼€å‘è€…: æ™ºå® (AIåŠ©æ‰‹)
"""

import asyncio
import sys
import aiohttp
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.crawler.bilibili.bilibili_crawler import BilibiliCrawler


class BilibiliZoneAnalyzer:
    """Bç«™åˆ†åŒºåˆ†æå™¨"""

    def __init__(self):
        self.crawler = BilibiliCrawler()
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        self.session = aiohttp.ClientSession(headers=headers)

    async def get_zone_videos(self, rid: int, num: int = 10) -> List[Dict]:
        """
        è·å–åˆ†åŒºçƒ­é—¨è§†é¢‘

        Args:
            rid: åˆ†åŒºID (11=ç§‘æŠ€, 36=æ•™è‚²)
            num: æ•°é‡

        Returns:
            è§†é¢‘åˆ—è¡¨
        """
        url = "https://api.bilibili.com/x/web-interface/ranking/v2"
        params = {
            'rid': rid,
            'type': 'all',
            'arc_type': 0
        }

        zone_name = "ç§‘æŠ€åŒº" if rid == 11 else "æ•™è‚²åŒº"
        print(f"\nè·å–{zone_name}æ’è¡Œæ¦œ...")

        try:
            async with self.session.get(url, params=params) as response:
                data = await response.json()

                if data.get('code') == 0:
                    items = data['data']['list'][:num]
                    print(f"âœ… è·å–åˆ° {len(items)} æ¡è§†é¢‘")
                    return items
                else:
                    print(f"âŒ APIé”™è¯¯: {data.get('message')}")
                    return []
        except Exception as e:
            print(f"âŒ è·å–{zone_name}å¤±è´¥: {e}")
            return []

    async def analyze_videos(self, videos: List[Dict], zone_name: str, max_count: int = 10) -> List[Dict]:
        """
        åˆ†æè§†é¢‘å†…å®¹

        Args:
            videos: è§†é¢‘åˆ—è¡¨
            zone_name: åˆ†åŒºåç§°
            max_count: æœ€å¤§åˆ†ææ•°é‡

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []

        for i, video in enumerate(videos[:max_count], 1):
            print(f"\n[{zone_name}] [{i}/{len(videos[:max_count])}] {video.get('title', '')[:50]}")

            bvid = video.get('bvid')

            if not bvid:
                continue

            try:
                video_data = await self.crawler.crawl_video_full(bvid)

                if video_data and video_data.get('video_info'):
                    info = video_data['video_info']

                    result = {
                        'rank': i,
                        'bvid': bvid,
                        'title': video.get('title', ''),  # ä½¿ç”¨åŸå§‹æ ‡é¢˜
                        'desc': video.get('description', '')[:300],
                        'author': video.get('owner', {}).get('name', ''),
                        'author_mid': video.get('owner', {}).get('mid', ''),
                        'play_count': info.get('play_count', video.get('stat', {}).get('view', 0)),
                        'like_count': info.get('like_count', video.get('stat', {}).get('like', 0)),
                        'coin_count': info.get('coin_count', video.get('stat', {}).get('coin', 0)),
                        'favorite_count': info.get('favorite_count', video.get('stat', {}).get('favorite', 0)),
                        'duration': video.get('duration', 0) // 60,  # è½¬ä¸ºåˆ†é’Ÿ
                        'category': video.get('tname', ''),
                        'pubdate': datetime.fromtimestamp(video.get('pubdate', 0)).strftime('%Y-%m-%d'),
                        'tags': []  # å¯ä»¥æ‰©å±•æ ‡ç­¾
                    }

                    results.append(result)
                    print(f"  âœ… æ’­æ”¾: {result['play_count']:,}")

                else:
                    # å¦‚æœAPIå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®
                    stat = video.get('stat', {})
                    result = {
                        'rank': i,
                        'bvid': bvid,
                        'title': video.get('title', ''),
                        'desc': video.get('description', '')[:300],
                        'author': video.get('owner', {}).get('name', ''),
                        'author_mid': video.get('owner', {}).get('mid', ''),
                        'play_count': stat.get('view', 0),
                        'like_count': stat.get('like', 0),
                        'coin_count': stat.get('coin', 0),
                        'favorite_count': stat.get('favorite', 0),
                        'duration': video.get('duration', 0) // 60,
                        'category': video.get('tname', ''),
                        'pubdate': datetime.fromtimestamp(video.get('pubdate', 0)).strftime('%Y-%m-%d'),
                        'tags': []
                    }
                    results.append(result)
                    print(f"  âš ï¸ ä½¿ç”¨åŸºç¡€æ•°æ®: {result['play_count']:,}")

            except Exception as e:
                print(f"  âŒ å¤±è´¥: {e}")

        return results

    async def close(self):
        """å…³é—­session"""
        await self.session.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Bç«™åˆ†åŒºå†…å®¹åˆ†æ - æ™ºå®å‡ºå“ ğŸŒ¸                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ç›®æ ‡:
1. çˆ¬å–Bç«™ç§‘æŠ€åŒºçƒ­é—¨è§†é¢‘ï¼ˆTOP 10ï¼‰
2. çˆ¬å–Bç«™æ•™è‚²åŒºçƒ­é—¨è§†é¢‘ï¼ˆTOP 10ï¼‰
3. åˆ†æå†…å®¹ç‰¹ç‚¹å’Œè¶‹åŠ¿
4. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    """)

    analyzer = BilibiliZoneAnalyzer()

    try:
        # çˆ¬å–ç§‘æŠ€åŒº
        print("\n" + "="*70)
        print("ğŸ”¬ ç§‘æŠ€åŒºåˆ†æ")
        print("="*70)

        tech_videos = await analyzer.get_zone_videos(rid=11, num=10)
        tech_results = await analyzer.analyze_videos(tech_videos, "ç§‘æŠ€åŒº", max_count=10)

        print(f"\nâœ… ç§‘æŠ€åŒºåˆ†æå®Œæˆ: {len(tech_results)} æ¡")

        # çˆ¬å–æ•™è‚²åŒº
        print("\n" + "="*70)
        print("ğŸ“š æ•™è‚²åŒºåˆ†æ")
        print("="*70)

        edu_videos = await analyzer.get_zone_videos(rid=36, num=10)
        edu_results = await analyzer.analyze_videos(edu_videos, "æ•™è‚²åŒº", max_count=10)

        print(f"\nâœ… æ•™è‚²åŒºåˆ†æå®Œæˆ: {len(edu_results)} æ¡")

        # ç”ŸæˆæŠ¥å‘Š
        print("\n" + "="*70)
        print("ğŸ“ ç”ŸæˆæŠ¥å‘Š")
        print("="*70)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = project_root / 'exports' / f'zone_analysis_{timestamp}.md'
        report_file.parent.mkdir(exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Bç«™åˆ†åŒºå†…å®¹åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**åˆ†æèŒƒå›´**: ç§‘æŠ€åŒºTOP 10 + æ•™è‚²åŒºTOP 10\n\n")
            f.write("---\n\n")

            # ç§‘æŠ€åŒºæŠ¥å‘Š
            f.write("## ğŸ”¬ ç§‘æŠ€åŒºåˆ†æ\n\n")

            if tech_results:
                total_plays = sum(r['play_count'] for r in tech_results)
                total_likes = sum(r['like_count'] for r in tech_results)

                f.write("### ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
                f.write(f"- **æ€»æ’­æ”¾é‡**: {total_plays:,}\n")
                f.write(f"- **æ€»ç‚¹èµæ•°**: {total_likes:,}\n")
                f.write(f"- **å¹³å‡æ’­æ”¾**: {total_plays//len(tech_results):,}\n")
                f.write(f"- **å¹³å‡ç‚¹èµ**: {total_likes//len(tech_results):,}\n\n")

                f.write("### ğŸ† çƒ­é—¨è§†é¢‘\n\n")

                for r in tech_results[:5]:
                    f.write(f"#### {r['rank']}. {r['title']}\n\n")
                    f.write(f"**UPä¸»**: {r['author']} | **å‘å¸ƒ**: {r['pubdate']}\n\n")
                    f.write(f"æ’­æ”¾: {r['play_count']:,} | ç‚¹èµ: {r['like_count']:,} | æŠ•å¸: {r['coin_count']:,}\n\n")
                    if r['desc']:
                        f.write(f"**ç®€ä»‹**: {r['desc'][:150]}...\n\n")
                    f.write("---\n\n")

            # æ•™è‚²åŒºæŠ¥å‘Š
            f.write("## ğŸ“š æ•™è‚²åŒºåˆ†æ\n\n")

            if edu_results:
                total_plays = sum(r['play_count'] for r in edu_results)
                total_likes = sum(r['like_count'] for r in edu_results)

                f.write("### ğŸ“Š æ€»ä½“ç»Ÿè®¡\n\n")
                f.write(f"- **æ€»æ’­æ”¾é‡**: {total_plays:,}\n")
                f.write(f"- **æ€»ç‚¹èµæ•°**: {total_likes:,}\n")
                f.write(f"- **å¹³å‡æ’­æ”¾**: {total_plays//len(edu_results):,}\n")
                f.write(f"- **å¹³å‡ç‚¹èµ**: {total_likes//len(edu_results):,}\n\n")

                f.write("### ğŸ† çƒ­é—¨è§†é¢‘\n\n")

                for r in edu_results[:5]:
                    f.write(f"#### {r['rank']}. {r['title']}\n\n")
                    f.write(f"**UPä¸»**: {r['author']} | **å‘å¸ƒ**: {r['pubdate']}\n\n")
                    f.write(f"æ’­æ”¾: {r['play_count']:,} | ç‚¹èµ: {r['like_count']:,} | æŠ•å¸: {r['coin_count']:,}\n\n")
                    if r['desc']:
                        f.write(f"**ç®€ä»‹**: {r['desc'][:150]}...\n\n")
                    f.write("---\n\n")

            # å¯¹æ¯”åˆ†æ
            f.write("## ğŸ“ˆ åˆ†åŒºå¯¹æ¯”\n\n")

            if tech_results and edu_results:
                tech_avg = tech_results[0]['play_count'] if tech_results else 0
                edu_avg = edu_results[0]['play_count'] if edu_results else 0

                f.write(f"- **ç§‘æŠ€åŒºTOP1æ’­æ”¾**: {tech_avg:,}\n")
                f.write(f"- **æ•™è‚²åŒºTOP1æ’­æ”¾**: {edu_avg:,}\n")

                if tech_avg > edu_avg:
                    ratio = tech_avg / edu_avg
                    f.write(f"- **å¯¹æ¯”**: ç§‘æŠ€åŒºæ˜¯æ•™è‚²åŒºçš„ {ratio:.1f} å€\n")
                else:
                    ratio = edu_avg / tech_avg
                    f.write(f"- **å¯¹æ¯”**: æ•™è‚²åŒºæ˜¯ç§‘æŠ€åŒºçš„ {ratio:.1f} å€\n")

        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

        # ä¿å­˜JSON
        json_file = project_root / 'exports' / f'zone_data_{timestamp}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'tech': tech_results,
                'education': edu_results
            }, f, ensure_ascii=False, indent=2)

        print(f"âœ… æ•°æ®å·²ä¿å­˜: {json_file}")

        print("\n" + "="*70)
        print("ğŸ‰ åˆ†æå®Œæˆï¼")
        print("="*70)

        # æ‰“å°ç®€è¦æ€»ç»“
        print(f"\nç§‘æŠ€åŒº: {len(tech_results)} æ¡")
        print(f"æ•™è‚²åŒº: {len(edu_results)} æ¡")
        print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_file}")

    finally:
        await analyzer.close()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
